{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9158ba78",
   "metadata": {},
   "source": [
    "# openai gym intro\n",
    "https://qiita.com/God_KonaBanana/items/c2cee09bc35cca722f2b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa2273",
   "metadata": {},
   "source": [
    "# DQN walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "42215594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "class drifters64():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.L=-2\n",
    "        self.counetr=0\n",
    "        self.x1,self.y1=1,1\n",
    "        self.vx1, self.vy1=0.5,1\n",
    "        \n",
    "        self.dt=1\n",
    "        self.vmax=30\n",
    "        #self.action_space=np.zeros((1,1))\n",
    "        #self.action_space=np.zeros(2)\n",
    "        self.action_space = gym.spaces.Discrete(3) # 行動空間。速度を下げる、そのまま、上げるの3種        \n",
    "        \n",
    "        self.img = cv2.imread(\"img//map64.bmp\", 0).astype(np.float32)/255.0   \n",
    "        self.img = self.img[:,:, np.newaxis]\n",
    "        self.observation_space=self.img #np.zeros(4)\n",
    "        #self.reward_range=np.zeros(2)\n",
    "        #self.metadata =np.zeros(2)\n",
    "        self.imgtmp = self.img.copy()        \n",
    "        self.idx = 0        \n",
    "        \n",
    "        #self.codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        #filepath   = '{:08}'.format(self.idx)+\"_.mp4\"\n",
    "        #self.video = cv2.VideoWriter(filepath, self.codec, 10, (1000, 1000))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.L=-2\n",
    "        self.counetr=0\n",
    "        self.x1,self.y1=1,1\n",
    "        self.vx,self.vy=0.5,1\n",
    "        \n",
    "        #self.obs1=np.array([self.x1,self.y1,self.vx,self.vy])\n",
    "        self.img = cv2.imread(\"img//map64.bmp\", 0).astype(np.float32)/255.0        \n",
    "        self.img = self.img[:,:, np.newaxis]        \n",
    "        self.imgtmp = self.img.copy()\n",
    "        #cv2.circle(self.imgtmp,(1,1), 2, 0.5 ,thickness=2)        \n",
    "        cv2.rectangle(self.imgtmp, (-1, -1), (3, 3), 0.5, thickness=-1)         \n",
    "        \n",
    "        self.obs1=self.imgtmp #np.zeros(4)\n",
    "\n",
    "        #cv2.rectangle(img, (0, 0), (2, 2), 0.5, thickness=-1) \n",
    "        \n",
    "        \n",
    "        #if self.idx % 256 == 0:\n",
    "        #    self.video.release()        \n",
    "        #    filepath = '{:08}'.format(self.idx)+\".mp4\"\n",
    "        #    self.video = cv2.VideoWriter(filepath, self.codec, 10, (1000, 1000))\n",
    "        print(np.shape(self.obs1))                \n",
    "        \n",
    "        return self.obs1\n",
    "    \n",
    "    def step(self, action, monitor_frame=False):\n",
    "        \n",
    "        print(action)        \n",
    "        action = action - 1\n",
    "        \n",
    "        #self.vx1 = self.vx1 + action[0] * self.dt\n",
    "        self.vx1 = 0.5*self.vx1 + action * self.dt        \n",
    "        #self.vx1 = action * self.dt                \n",
    "        self.x1 = self.x1 + self.vx1 * self.dt\n",
    "        \n",
    "        self.vy1 = self.vy1 #+ action[1] * self.dt\n",
    "        self.y1 = self.y1 + self.vy1 * self.dt\n",
    "        \n",
    "        self.counetr+=1\n",
    "        #self.L+=self.dt        \n",
    "            \n",
    "        done=False            \n",
    "        \n",
    "        if self.x1 < 0 or self.y1 < 0 or self.x1 > 63  or self.img[max(0,min(63,int(self.y1))),max(0,min(63,int(self.x1)))]==0:\n",
    "            self.L+=-10000\n",
    "            done=True\n",
    "            \n",
    "        else:\n",
    "            done=False\n",
    "\n",
    "            if self.y1 > 60 and self.x1 > 60:            \n",
    "                self.L+=1000\n",
    "                done=True\n",
    "            \n",
    "            else:\n",
    "                self.imgtmp = self.img.copy()            \n",
    "                #cv2.circle(self.imgtmp,(int(self.x1),int(self.y1)), 2, 0.5 ,thickness=2)        \n",
    "                cv2.rectangle(self.imgtmp, (int(self.x1)-2, int(self.y1)-2), (int(self.x1)+2, int(self.y1)+2), 0.5, thickness=-1)\n",
    "                \n",
    "                #self.L+=0.2*(1 + self.y1*0.01)              \n",
    "                self.L+=0.1*(1 + self.y1*0.02 + self.x1*0.02)                              \n",
    "            \n",
    "        self.reward=self.L\n",
    "        \n",
    "        self.obs1=self.imgtmp #np.array([self.x1,self.y1,self.vx,self.vy])\n",
    "        #print(np.shape(self.obs1))\n",
    "        \n",
    "        #if monitor_frame:\n",
    "        if True:        \n",
    "\n",
    "            imgtmp_bgr = cv2.cvtColor(self.imgtmp, cv2.COLOR_GRAY2BGR)\n",
    "            cv2.rectangle(imgtmp_bgr, (int(self.x1)-2, int(self.y1)-2), (int(self.x1)+2, int(self.y1)+2), (0,0,1), thickness=-1)\n",
    "                \n",
    "            cv2.imwrite(\"video/\" + '{:08}'.format(self.idx)+\".jpg\" , imgtmp_bgr*255)\n",
    "            self.idx+=1\n",
    "            \n",
    "            \n",
    "        #print(\"step\")\n",
    "        return self.obs1, self.reward, done, {}\n",
    "    \n",
    "    def render(self, mode='rgb_array'):\n",
    "        \n",
    "        print(\"monitor_frame test\")\n",
    "        self.imgtmp = self.img.copy()            \n",
    "        #cv2.circle(self.imgtmp,(int(self.x1),int(self.y1)), 2, 0.5 ,thickness=2)        \n",
    "        cv2.rectangle(self.imgtmp, (int(self.x1)-2, int(self.y1)-2), (int(self.x1)+2, int(self.y1)+2), 0.5, thickness=-1)\n",
    "        \n",
    "        cv2.imwrite(\"video/\" + '{:08}'.format(self.idx)+\".jpg\" , self.imgtmp*255)\n",
    "        self.idx+=1\n",
    "                    \n",
    "        #self.video.write(img_out)\n",
    "                    \n",
    "        return self.imgtmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c68087a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.action_space.shape\n",
      "()\n",
      "nb_actions:  3\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_165 (Conv2D)          (None, 1, 15, 15, 32)     2080      \n",
      "_________________________________________________________________\n",
      "activation_328 (Activation)  (None, 1, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_166 (Conv2D)          (None, 1, 6, 6, 16)       8208      \n",
      "_________________________________________________________________\n",
      "activation_329 (Activation)  (None, 1, 6, 6, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_167 (Conv2D)          (None, 1, 4, 4, 16)       2320      \n",
      "_________________________________________________________________\n",
      "activation_330 (Activation)  (None, 1, 4, 4, 16)       0         \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_331 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "activation_332 (Activation)  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_333 (Activation)  (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 177,219\n",
      "Trainable params: 177,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\py36\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 8000 steps ...\n",
      "(64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\py36\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "   23/8000: episode: 1, duration: 20.858s, episode steps:  23, steps per second:   1, episode reward: -9305.560, mean reward: -404.590 [-9942.360, 63.579], mean action: 1.609 [0.000, 2.000],  loss: 223.583669, mae: 7.513526, mean_q: 8.678075\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\py36\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "   46/8000: episode: 2, duration: 3.295s, episode steps:  23, steps per second:   7, episode reward: -9260.811, mean reward: -402.644 [-9940.500, 65.770], mean action: 1.348 [0.000, 2.000],  loss: 1951404.875000, mae: 140.111237, mean_q: 6.738910\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "   64/8000: episode: 3, duration: 2.535s, episode steps:  18, steps per second:   7, episode reward: -9623.497, mean reward: -534.639 [-9959.263, 40.737], mean action: 0.944 [0.000, 2.000],  loss: 1802168.500000, mae: 129.774963, mean_q: 0.077685\n",
      "(64, 64, 1)\n",
      "0\n",
      "   65/8000: episode: 4, duration: 0.291s, episode steps:   1, steps per second:   3, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 4638829.500000, mae: 319.917297, mean_q: 0.093280\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "0\n",
      "   68/8000: episode: 5, duration: 0.533s, episode steps:   3, steps per second:   6, episode reward: -9995.347, mean reward: -3331.782 [-9997.738,  2.262], mean action: 0.667 [0.000, 2.000],  loss: 4132940.750000, mae: 284.478546, mean_q: 0.115635\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "   70/8000: episode: 6, duration: 0.386s, episode steps:   2, steps per second:   5, episode reward: -9999.818, mean reward: -4999.909 [-9999.909,  0.091], mean action: 1.000 [1.000, 1.000],  loss: 2337754.750000, mae: 164.359955, mean_q: 0.182260\n",
      "(64, 64, 1)\n",
      "0\n",
      "   71/8000: episode: 7, duration: 0.265s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 4669274.500000, mae: 321.117249, mean_q: 0.266600\n",
      "(64, 64, 1)\n",
      "0\n",
      "   72/8000: episode: 8, duration: 0.275s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 3113119.000000, mae: 217.747284, mean_q: 0.313582\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "   74/8000: episode: 9, duration: 0.399s, episode steps:   2, steps per second:   5, episode reward: -9999.824, mean reward: -4999.912 [-9999.912,  0.088], mean action: 1.000 [1.000, 1.000],  loss: 6996617.000000, mae: 475.909149, mean_q: 0.369054\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  136/8000: episode: 10, duration: 8.899s, episode steps:  62, steps per second:   7, episode reward: 17990.118, mean reward: 290.163 [ 0.112, 10549.611], mean action: 1.500 [0.000, 2.000],  loss: 4020143.000000, mae: 1110.872925, mean_q: -276.008026\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  168/8000: episode: 11, duration: 4.423s, episode steps:  32, steps per second:   7, episode reward: -8253.600, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 2459686.000000, mae: 1164.072266, mean_q: 18.272158\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  200/8000: episode: 12, duration: 4.454s, episode steps:  32, steps per second:   7, episode reward: -8253.600, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 2673709.000000, mae: 1029.413330, mean_q: -23.237698\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  232/8000: episode: 13, duration: 4.747s, episode steps:  32, steps per second:   7, episode reward: -8253.600, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 2630640.500000, mae: 1420.116943, mean_q: 6.205653\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  301/8000: episode: 14, duration: 10.261s, episode steps:  69, steps per second:   7, episode reward: 21978.480, mean reward: 318.529 [ 0.200, 10653.340], mean action: 1.435 [1.000, 2.000],  loss: 2479198.500000, mae: 1249.009888, mean_q: -269.199158\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  333/8000: episode: 15, duration: 5.751s, episode steps:  32, steps per second:   6, episode reward: -8253.605, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 1721149.500000, mae: 1053.340088, mean_q: -44.492119\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  365/8000: episode: 16, duration: 7.955s, episode steps:  32, steps per second:   4, episode reward: -8253.600, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 2238614.250000, mae: 1239.624756, mean_q: -138.590271\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  397/8000: episode: 17, duration: 7.254s, episode steps:  32, steps per second:   4, episode reward: -8253.600, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 1614536.750000, mae: 1241.825439, mean_q: -248.494751\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  429/8000: episode: 18, duration: 5.192s, episode steps:  32, steps per second:   6, episode reward: -8253.600, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 2103283.000000, mae: 1285.295898, mean_q: -188.341064\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "  452/8000: episode: 19, duration: 3.283s, episode steps:  23, steps per second:   7, episode reward: -9251.840, mean reward: -402.254 [-9939.520, 66.760], mean action: 1.348 [1.000, 2.000],  loss: 1866379.500000, mae: 1140.447632, mean_q: -181.661392\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "  502/8000: episode: 20, duration: 7.112s, episode steps:  50, steps per second:   7, episode reward: -6291.997, mean reward: -125.840 [-9842.200, 163.320], mean action: 1.100 [1.000, 2.000],  loss: 1741909.250000, mae: 1157.689209, mean_q: -289.590302\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  525/8000: episode: 21, duration: 3.810s, episode steps:  23, steps per second:   6, episode reward: -9382.640, mean reward: -407.941 [-9950.800, 55.639], mean action: 1.348 [1.000, 2.000],  loss: 1665962.250000, mae: 1320.249634, mean_q: -404.338867\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  557/8000: episode: 22, duration: 4.549s, episode steps:  32, steps per second:   7, episode reward: -8253.755, mean reward: -257.930 [-9878.009, 121.991], mean action: 2.000 [2.000, 2.000],  loss: 1791821.750000, mae: 1616.770142, mean_q: -416.206451\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  624/8000: episode: 23, duration: 9.917s, episode steps:  67, steps per second:   7, episode reward: 21203.280, mean reward: 316.467 [ 0.200, 10636.360], mean action: 1.448 [1.000, 2.000],  loss: 1531655.250000, mae: 1263.963745, mean_q: -319.737701\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "  647/8000: episode: 24, duration: 3.285s, episode steps:  23, steps per second:   7, episode reward: -9224.325, mean reward: -401.058 [-9935.520, 70.440], mean action: 1.522 [1.000, 2.000],  loss: 1666898.625000, mae: 1371.702759, mean_q: -354.185089\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  670/8000: episode: 25, duration: 3.293s, episode steps:  23, steps per second:   7, episode reward: -9382.630, mean reward: -407.940 [-9950.799, 55.640], mean action: 1.348 [1.000, 2.000],  loss: 1455258.125000, mae: 1334.142700, mean_q: -344.489014\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  732/8000: episode: 26, duration: 8.971s, episode steps:  62, steps per second:   7, episode reward: 26264.259, mean reward: 423.617 [ 0.200, 10965.299], mean action: 1.484 [1.000, 2.000],  loss: 1256067.125000, mae: 1307.859009, mean_q: -321.374115\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "  766/8000: episode: 27, duration: 4.828s, episode steps:  34, steps per second:   7, episode reward: -8505.391, mean reward: -250.159 [-9908.768, 91.232], mean action: 0.941 [0.000, 2.000],  loss: 1668698.750000, mae: 1137.212524, mean_q: -428.254608\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "  783/8000: episode: 28, duration: 2.444s, episode steps:  17, steps per second:   7, episode reward: -9684.392, mean reward: -569.670 [-9963.791, 36.209], mean action: 0.941 [0.000, 1.000],  loss: 2210631.500000, mae: 1257.232178, mean_q: -439.322052\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  818/8000: episode: 29, duration: 4.869s, episode steps:  35, steps per second:   7, episode reward: -8030.560, mean reward: -229.445 [-9875.200, 128.880], mean action: 1.914 [0.000, 2.000],  loss: 1718158.625000, mae: 1094.431641, mean_q: -430.034912\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "  890/8000: episode: 30, duration: 10.798s, episode steps:  72, steps per second:   7, episode reward: 21044.800, mean reward: 292.289 [ 0.200, 10629.560], mean action: 1.417 [0.000, 2.000],  loss: 1434940.875000, mae: 1181.054443, mean_q: -455.448029\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "  913/8000: episode: 31, duration: 3.282s, episode steps:  23, steps per second:   7, episode reward: -9251.840, mean reward: -402.254 [-9939.520, 66.760], mean action: 1.348 [1.000, 2.000],  loss: 1390342.875000, mae: 995.383728, mean_q: -414.923096\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "  946/8000: episode: 32, duration: 5.967s, episode steps:  33, steps per second:   6, episode reward: -8602.089, mean reward: -260.669 [-9915.270, 90.740], mean action: 1.242 [0.000, 2.000],  loss: 1797198.750000, mae: 1128.435913, mean_q: -510.422302\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1083/8000: episode: 33, duration: 33.045s, episode steps: 137, steps per second:   4, episode reward: 54882.889, mean reward: 400.605 [ 0.185, 11227.046], mean action: 1.219 [0.000, 2.000],  loss: 1542950.250000, mae: 1026.181519, mean_q: -465.167023\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1115/8000: episode: 34, duration: 7.199s, episode steps:  32, steps per second:   4, episode reward: -8253.605, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 1548795.000000, mae: 767.765991, mean_q: -339.007568\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      " 1138/8000: episode: 35, duration: 3.551s, episode steps:  23, steps per second:   6, episode reward: -9265.840, mean reward: -402.863 [-9941.520, 64.920], mean action: 1.261 [0.000, 2.000],  loss: 1526917.250000, mae: 962.115234, mean_q: -487.663086\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1161/8000: episode: 36, duration: 3.950s, episode steps:  23, steps per second:   6, episode reward: -9333.523, mean reward: -405.805 [-9945.443, 60.755], mean action: 1.478 [1.000, 2.000],  loss: 1472194.125000, mae: 1012.776978, mean_q: -496.959412\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1193/8000: episode: 37, duration: 5.821s, episode steps:  32, steps per second:   5, episode reward: -8254.812, mean reward: -257.963 [-9878.073, 121.927], mean action: 2.000 [2.000, 2.000],  loss: 1249910.875000, mae: 699.384033, mean_q: -353.369812\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      " 1216/8000: episode: 38, duration: 3.656s, episode steps:  23, steps per second:   6, episode reward: -9219.473, mean reward: -400.847 [-9934.767, 71.146], mean action: 1.565 [0.000, 2.000],  loss: 1547566.750000, mae: 860.892639, mean_q: -477.861908\n",
      "(64, 64, 1)\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      " 1222/8000: episode: 39, duration: 1.084s, episode steps:   6, steps per second:   6, episode reward: -9968.862, mean reward: -1661.477 [-9991.119,  8.881], mean action: 0.833 [0.000, 2.000],  loss: 614162.937500, mae: 883.376221, mean_q: -528.155334\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1245/8000: episode: 40, duration: 3.254s, episode steps:  23, steps per second:   7, episode reward: -9345.456, mean reward: -406.324 [-9944.766, 61.120], mean action: 1.652 [1.000, 2.000],  loss: 1138166.250000, mae: 601.009766, mean_q: -308.539642\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      " 1283/8000: episode: 41, duration: 5.906s, episode steps:  38, steps per second:   6, episode reward: -7408.532, mean reward: -194.961 [-9852.790, 151.660], mean action: 1.632 [0.000, 2.000],  loss: 1552836.000000, mae: 730.629761, mean_q: -391.715088\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1333/8000: episode: 42, duration: 9.883s, episode steps:  50, steps per second:   5, episode reward: -6483.805, mean reward: -129.676 [-9854.071, 151.478], mean action: 1.100 [0.000, 2.000],  loss: 1690213.875000, mae: 885.118103, mean_q: -488.958130\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      " 1356/8000: episode: 43, duration: 5.339s, episode steps:  23, steps per second:   4, episode reward: -9348.080, mean reward: -406.438 [-9946.905, 59.459], mean action: 1.304 [1.000, 2.000],  loss: 1373849.750000, mae: 776.024475, mean_q: -434.339539\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      " 1406/8000: episode: 44, duration: 8.623s, episode steps:  50, steps per second:   6, episode reward: -6570.959, mean reward: -131.419 [-9856.878, 149.039], mean action: 1.000 [1.000, 1.000],  loss: 1380441.625000, mae: 843.537415, mean_q: -457.696259\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1438/8000: episode: 45, duration: 4.995s, episode steps:  32, steps per second:   6, episode reward: -8293.280, mean reward: -259.165 [-9880.400, 119.600], mean action: 2.000 [2.000, 2.000],  loss: 1848714.625000, mae: 822.265320, mean_q: -369.855957\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      " 1470/8000: episode: 46, duration: 4.396s, episode steps:  32, steps per second:   7, episode reward: -8253.600, mean reward: -257.925 [-9878.000, 122.000], mean action: 2.000 [2.000, 2.000],  loss: 1346315.000000, mae: 864.209595, mean_q: -429.676270\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      " 1495/8000: episode: 47, duration: 3.435s, episode steps:  25, steps per second:   7, episode reward: -9166.414, mean reward: -366.657 [-9938.066, 68.267], mean action: 1.280 [0.000, 2.000],  loss: 1923638.125000, mae: 1098.503174, mean_q: -604.542480\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      " 1518/8000: episode: 48, duration: 3.259s, episode steps:  23, steps per second:   7, episode reward: -9255.899, mean reward: -402.430 [-9939.073, 66.982], mean action: 1.478 [0.000, 2.000],  loss: 2028469.625000, mae: 879.715271, mean_q: -467.025299\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "0\n",
      " 1521/8000: episode: 49, duration: 0.683s, episode steps:   3, steps per second:   4, episode reward: -9995.195, mean reward: -3331.732 [-9997.675,  2.325], mean action: 0.667 [0.000, 2.000],  loss: 166628.796875, mae: 972.872620, mean_q: -607.151062\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      " 1524/8000: episode: 50, duration: 0.535s, episode steps:   3, steps per second:   6, episode reward: -9995.474, mean reward: -3331.825 [-9997.784,  2.216], mean action: 1.000 [1.000, 1.000],  loss: 524973.875000, mae: 870.255066, mean_q: -542.426331\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      " 1574/8000: episode: 51, duration: 7.202s, episode steps:  50, steps per second:   7, episode reward: -6581.577, mean reward: -131.632 [-9857.145, 148.711], mean action: 1.020 [1.000, 2.000],  loss: 1504387.500000, mae: 786.061707, mean_q: -475.657959\n",
      "(64, 64, 1)\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      " 1594/8000: episode: 52, duration: 2.794s, episode steps:  20, steps per second:   7, episode reward: -9538.420, mean reward: -476.921 [-9954.093, 45.907], mean action: 0.950 [0.000, 2.000],  loss: 1212954.250000, mae: 979.148743, mean_q: -631.497192\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      " 1644/8000: episode: 53, duration: 7.009s, episode steps:  50, steps per second:   7, episode reward: -6492.848, mean reward: -129.857 [-9850.305, 155.205], mean action: 1.120 [0.000, 2.000],  loss: 1831318.875000, mae: 863.490234, mean_q: -574.092529\n",
      "(64, 64, 1)\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      " 1670/8000: episode: 54, duration: 4.209s, episode steps:  26, steps per second:   6, episode reward: -9170.489, mean reward: -352.711 [-9934.899, 65.101], mean action: 0.923 [0.000, 2.000],  loss: 1060532.250000, mae: 835.515747, mean_q: -596.653992\n",
      "(64, 64, 1)\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      " 1704/8000: episode: 55, duration: 4.638s, episode steps:  34, steps per second:   7, episode reward: -8434.206, mean reward: -248.065 [-9907.160, 98.779], mean action: 1.265 [0.000, 2.000],  loss: 1472223.375000, mae: 836.729309, mean_q: -604.115051\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      " 1712/8000: episode: 56, duration: 1.202s, episode steps:   8, steps per second:   7, episode reward: -9936.719, mean reward: -1242.090 [-9986.003, 13.997], mean action: 0.625 [0.000, 2.000],  loss: 1959820.250000, mae: 936.283936, mean_q: -624.802856\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      " 1718/8000: episode: 57, duration: 0.930s, episode steps:   6, steps per second:   6, episode reward: -9968.011, mean reward: -1661.335 [-9990.925,  9.075], mean action: 0.833 [0.000, 2.000],  loss: 2595986.000000, mae: 1060.547852, mean_q: -708.551819\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1719/8000: episode: 58, duration: 0.265s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 122534.578125, mae: 1072.061279, mean_q: -858.906250\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      " 1742/8000: episode: 59, duration: 3.150s, episode steps:  23, steps per second:   7, episode reward: -9358.367, mean reward: -406.886 [-9944.537, 55.463], mean action: 1.000 [0.000, 2.000],  loss: 1877543.000000, mae: 1002.319702, mean_q: -712.230530\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      " 1754/8000: episode: 60, duration: 1.758s, episode steps:  12, steps per second:   7, episode reward: -9843.948, mean reward: -820.329 [-9975.803, 24.197], mean action: 0.917 [0.000, 2.000],  loss: 1773927.125000, mae: 951.997559, mean_q: -692.535828\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      " 1758/8000: episode: 61, duration: 0.749s, episode steps:   4, steps per second:   5, episode reward: -9988.619, mean reward: -2497.155 [-9995.528,  4.472], mean action: 0.750 [0.000, 2.000],  loss: 1069722.750000, mae: 1039.533081, mean_q: -801.353882\n",
      "(64, 64, 1)\n",
      "1\n",
      "0\n",
      " 1760/8000: episode: 62, duration: 0.536s, episode steps:   2, steps per second:   4, episode reward: -9999.807, mean reward: -4999.904 [-9999.904,  0.096], mean action: 0.500 [0.000, 1.000],  loss: 660418.625000, mae: 949.811829, mean_q: -776.959473\n",
      "(64, 64, 1)\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      " 1765/8000: episode: 63, duration: 0.918s, episode steps:   5, steps per second:   5, episode reward: -9979.638, mean reward: -1995.928 [-9993.294,  6.706], mean action: 0.800 [0.000, 2.000],  loss: 1967010.625000, mae: 930.700378, mean_q: -716.251343\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1766/8000: episode: 64, duration: 0.267s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 1319086.375000, mae: 891.997009, mean_q: -720.000977\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "1\n",
      " 1769/8000: episode: 65, duration: 0.535s, episode steps:   3, steps per second:   6, episode reward: -9995.354, mean reward: -3331.785 [-9997.741,  2.259], mean action: 1.000 [0.000, 2.000],  loss: 937832.250000, mae: 940.550964, mean_q: -762.061462\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      " 1779/8000: episode: 66, duration: 1.453s, episode steps:  10, steps per second:   7, episode reward: -9900.093, mean reward: -990.009 [-9981.625, 18.375], mean action: 0.900 [0.000, 1.000],  loss: 939496.000000, mae: 843.757629, mean_q: -697.634644\n",
      "(64, 64, 1)\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      " 1784/8000: episode: 67, duration: 0.791s, episode steps:   5, steps per second:   6, episode reward: -9979.920, mean reward: -1995.984 [-9993.343,  6.657], mean action: 0.800 [0.000, 2.000],  loss: 827464.687500, mae: 710.717590, mean_q: -625.340210\n",
      "(64, 64, 1)\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      " 1788/8000: episode: 68, duration: 0.686s, episode steps:   4, steps per second:   6, episode reward: -9988.548, mean reward: -2497.137 [-9995.499,  4.501], mean action: 0.750 [0.000, 2.000],  loss: 1101983.250000, mae: 690.737488, mean_q: -598.006714\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1789/8000: episode: 69, duration: 0.259s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 1376404.250000, mae: 715.146667, mean_q: -575.528748\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1790/8000: episode: 70, duration: 0.269s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 5261380.000000, mae: 972.958984, mean_q: -558.567078\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1791/8000: episode: 71, duration: 0.256s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 3552048.000000, mae: 852.236694, mean_q: -591.006958\n",
      "(64, 64, 1)\n",
      "1\n",
      "0\n",
      " 1793/8000: episode: 72, duration: 0.376s, episode steps:   2, steps per second:   5, episode reward: -9999.837, mean reward: -4999.919 [-9999.919,  0.081], mean action: 0.500 [0.000, 1.000],  loss: 1883003.625000, mae: 840.787292, mean_q: -650.601440\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1794/8000: episode: 73, duration: 0.271s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 2491103.750000, mae: 1007.448730, mean_q: -737.530701\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1795/8000: episode: 74, duration: 0.261s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 3363859.500000, mae: 1193.937744, mean_q: -844.717529\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1796/8000: episode: 75, duration: 0.263s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 2438144.500000, mae: 1167.301147, mean_q: -872.654541\n",
      "(64, 64, 1)\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      " 1800/8000: episode: 76, duration: 0.688s, episode steps:   4, steps per second:   6, episode reward: -9988.704, mean reward: -2497.176 [-9995.555,  4.445], mean action: 1.000 [0.000, 2.000],  loss: 1732906.125000, mae: 1199.977051, mean_q: -933.556641\n",
      "(64, 64, 1)\n",
      "1\n",
      "0\n",
      " 1802/8000: episode: 77, duration: 0.380s, episode steps:   2, steps per second:   5, episode reward: -9999.780, mean reward: -4999.890 [-9999.890,  0.110], mean action: 0.500 [0.000, 1.000],  loss: 4424155.500000, mae: 1440.871338, mean_q: -981.470032\n",
      "(64, 64, 1)\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      " 1825/8000: episode: 78, duration: 3.164s, episode steps:  23, steps per second:   7, episode reward: -9333.891, mean reward: -405.821 [-9946.598, 59.799], mean action: 1.391 [0.000, 2.000],  loss: 2355737.500000, mae: 1235.275024, mean_q: -848.357605\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      " 1836/8000: episode: 79, duration: 1.595s, episode steps:  11, steps per second:   7, episode reward: -9867.874, mean reward: -897.079 [-9977.943, 22.057], mean action: 0.818 [0.000, 2.000],  loss: 1946292.500000, mae: 1584.650513, mean_q: -995.757080\n",
      "(64, 64, 1)\n",
      "1\n",
      "0\n",
      " 1838/8000: episode: 80, duration: 0.399s, episode steps:   2, steps per second:   5, episode reward: -9999.793, mean reward: -4999.896 [-9999.896,  0.104], mean action: 0.500 [0.000, 1.000],  loss: 588562.187500, mae: 899.742859, mean_q: -549.377075\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      " 1872/8000: episode: 81, duration: 5.004s, episode steps:  34, steps per second:   7, episode reward: -8329.745, mean reward: -244.992 [-9903.170, 102.776], mean action: 1.265 [0.000, 2.000],  loss: 1671608.500000, mae: 1126.848389, mean_q: -616.100464\n",
      "(64, 64, 1)\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      " 1883/8000: episode: 82, duration: 1.604s, episode steps:  11, steps per second:   7, episode reward: -9872.053, mean reward: -897.459 [-9978.465, 21.535], mean action: 0.818 [0.000, 2.000],  loss: 1691723.125000, mae: 849.305481, mean_q: -562.970276\n",
      "(64, 64, 1)\n",
      "0\n",
      " 1884/8000: episode: 83, duration: 0.261s, episode steps:   1, steps per second:   4, episode reward: -10002.000, mean reward: -10002.000 [-10002.000, -10002.000], mean action: 0.000 [0.000, 0.000],  loss: 1397416.250000, mae: 861.449585, mean_q: -611.638916\n",
      "(64, 64, 1)\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      " 1890/8000: episode: 84, duration: 0.915s, episode steps:   6, steps per second:   7, episode reward: -9967.971, mean reward: -1661.329 [-9990.931,  9.069], mean action: 1.000 [0.000, 2.000],  loss: 2136270.750000, mae: 1006.146240, mean_q: -702.603699\n",
      "(64, 64, 1)\n",
      "1\n",
      "0\n",
      " 1892/8000: episode: 85, duration: 0.392s, episode steps:   2, steps per second:   5, episode reward: -9999.772, mean reward: -4999.886 [-9999.886,  0.114], mean action: 0.500 [0.000, 1.000],  loss: 4108262.500000, mae: 1359.504883, mean_q: -858.279114\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      " 1903/8000: episode: 86, duration: 1.621s, episode steps:  11, steps per second:   7, episode reward: -9875.447, mean reward: -897.768 [-9978.996, 21.004], mean action: 0.909 [0.000, 2.000],  loss: 1161238.000000, mae: 1385.166138, mean_q: -989.421509\n",
      "(64, 64, 1)\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "done, took 337.721 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Concatenate, Conv2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "import cv2\n",
    "#env = drifters()#自作ゲームのプログラムを環境とする\n",
    "env = drifters64()#自作ゲームのプログラムを環境とする\n",
    "\n",
    "# save video \n",
    "#from gym import wrappers\n",
    "#env = wrappers.Monitor(env, \"C://code//gym//video\", force=True)\n",
    "\n",
    "print(\"env.action_space.shape\")\n",
    "print(env.action_space.shape)\n",
    "#nb_actions = env.action_space.shape[0]\n",
    "nb_actions = env.action_space.n\n",
    "#nb_actions = env.action_space.n\n",
    "#nb_actions = len(env.action_space.shape)\n",
    "print(\"nb_actions: \", nb_actions)\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Input(input_shape=(1,) + env.observation_space.shape))\n",
    "#model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "#model.add(Conv2D(32, (8, 8), strides=(4, 4), input_shape=(128, 128, 1)))\n",
    "\n",
    "#model.add(Permute((2, 3, 1), input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4), input_shape=(2,) + env.observation_space.shape))\n",
    "#model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "#model.add(Permute((2, 3, 1)))  # (window,w,h) -> (w,h,window)\n",
    "\n",
    "#model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, (4, 4), strides=(2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(16, (3, 3), strides=(1, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# experience replay用のmemory\n",
    "memory = SequentialMemory(limit=50000, window_length=2)\n",
    "# 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "policy = EpsGreedyQPolicy(eps=0.1) \n",
    "#policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "#dqn.load_weights(\"dqn_model_weight_h5.h5\")\n",
    "\n",
    "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2, nb_max_episode_steps=300)\n",
    "#学習の様子を描画したいときは、Envに_render()を実装して、visualize=True にします,\n",
    "#dqn.test(env, nb_episodes=1, visualize=False, nb_max_episode_steps=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(\"dqn_model_weight_h5.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4357c5d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 結果を表示\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history[\"nb_episode_steps\"])\n",
    "plt.ylabel(\"step\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history[\"episode_reward\"])\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "\n",
    "plt.show()  # windowが表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a05b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
